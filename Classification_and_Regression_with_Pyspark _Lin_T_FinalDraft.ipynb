{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do Classification and Regression with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Supported by TingLin**\n",
    "\n",
    "- **Kansas State University**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "- **Classification**\n",
    "\n",
    "  - [Logistic regression](#Logistic regression )\n",
    "    \n",
    "    - [Binomial logistic regression](#Binomial logistic regression )\n",
    "    \n",
    "    - [Multinomial logistic regression](#Multinomial logistic regression)\n",
    "    \n",
    "  - [Decision tree classifier](#Decision tree classifier)\n",
    "    \n",
    "  - [Random forest classifier](#Random forest classifier)\n",
    "    \n",
    "  - [Gradient-boosted tree classifier](#Gradient-boosted tree classifier)\n",
    "    \n",
    "  - [Multilayer perceptron classifier](#Multilayer perceptron classifier)\n",
    "    \n",
    "  - [Linear Support Vector Machine](#Linear Support Vector Machine)\n",
    "    \n",
    "  - [One-vs-Rest classifier (a.k.a. One-vs-All)](#One-vs-Rest classifier)\n",
    "    \n",
    "  - [Naive Bayes](#Naive Bayes)\n",
    "    \n",
    "- ** Regression**\n",
    "\n",
    "  - [Linear regression](#Linear regression)\n",
    "    \n",
    "  - [Generalized linear regression](#Generalized linear regression)\n",
    "    \n",
    "  - [Available families](#Available families)\n",
    "    \n",
    "  - [Decision tree regression](#Decision tree regression)\n",
    "    \n",
    "  - [Random forest regression](#Random forest regression)\n",
    "    \n",
    "  - [Gradient-boosted tree regression](#Gradient-boosted tree regression)\n",
    "    \n",
    "  - [Survival regression](#Survival regression)\n",
    "    \n",
    "  - [Isotonic regression](#Isotonic regression)\n",
    "      \n",
    "- **How to do classification and build regression model for csv file**\n",
    "  - [Binary Classification](#Binary Classification)\n",
    "  - [Regression](#Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What am I going to learn from this Pyspark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apache Spark :\n",
    "    -is a must for Big data’s lovers. In a few words, Spark is a fast and powerful framework that provides an API to perform massive distributed processing over resilient sets of data.\n",
    "    - Apache Spark supports three most powerful programming languages:\n",
    "        - 1. Scala\n",
    "        - 2. Java\n",
    "        - 3. Python\n",
    "- Apache Spark is written in Scala programming language that compiles the program code into byte code for the JVM for spark big data processing. The open source community has developed a wonderful utility for spark python big data processing known as PySpark. PySpark helps data scientists interface with Resilient Distributed Datasets in apache spark and python.Py4J is a popularly library integrated within PySpark that lets python interface dynamically with JVM objects (RDD’s).\n",
    "- Apache Spark comes with an interactive shell for python as it does for Scala. The shell for python is known as “PySpark”. To use PySpark you will have to have python installed on your machine. As we know that each Linux machine comes preinstalled with python so you need not worry about python installation. To get started in a standalone mode you can download the pre-built version of spark from its official home page listed in the pre-requisites section of the PySpark tutorial. \n",
    "- Decompress the downloaded file. On decompressing the spark downloadable, you will see the following structure:\n",
    "    - bin: Holds all the binaries\n",
    "    - conf: Holds all the necessary configuration files to run any spark application\n",
    "    - ec2: Holds the scripts to launch a cluster on amazon cloud space with multiple ec2 instances\n",
    "    - lib: Holds the prebuilt libraries which make up the spark APIS.\n",
    "    - Licenses:\n",
    "        - python\n",
    "        - python API\n",
    "        - README.md:\n",
    "            - Holds important instructions to get started with spark\n",
    "            - Holds important startup scripts that are required to setup distributed clustter\n",
    "        - CHANGES.txt:\n",
    "            - Holds all the changes information for each version of apache spark\n",
    "        - data:\n",
    "            - Holds data that is used in the examples\n",
    "        - examples:\n",
    "            - Has examples which are a good place to learn the usage of spark functions\n",
    "        - LICENSE NOTICE:\n",
    "            - Important information\n",
    "        - R:\n",
    "            - Holds API of R language\n",
    "        - RELEASE:\n",
    "            - Holds make info of the downloaded version        \n",
    "- Jupyter notebook:\n",
    "    -is a popular application that enables you to edit, run and share Python code into a web view. It allows you to modify and re-execute parts of your code in a very flexible way. \n",
    "    -That’s why Jupyter is a great tool to test and prototype programs.\n",
    "- Python:\n",
    "    - is a powerful programming language for handling complex data analysis and data munging tasks. It has several in-built libraries and frameworks to do data mining tasks efficiently. However, no programming language alone can handle big data processing efficiently. There is always need for a distributed computing framework like Hadoop or Spark.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification and regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "import pyspark.sql \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from operator import itemgetter\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Pysaprk, sqlContext is used as spark, it can read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification\n",
    "<a id = 'Logistic regression'></a>\n",
    "- ** 2.1 Logistic regression**\n",
    "<a id = 'Binomial logistic regression'></a>\n",
    "    - 2.1.1 Binomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\n",
      "Intercept: 0.224563159613\n",
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.120658794459,0.120658794459]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression is a popular method to predict a categorical response. It is a special case of Generalized Linear models that predicts the probability of the outcomes. \n",
    "- elasticNetParam corresponds to α and regParam corresponds to λ.\n",
    "- Multinomial logistic regression can be used for binary classification by setting the family param to “multinomial”. It will produce two sets of coefficients and two intercepts.\n",
    "- When fitting LogisticRegressionModel without intercept on dataset with constant nonzero column, Spark MLlib outputs zero coefficients for constant nonzero columns. \n",
    "- In spark.ml, we also include Pipelines API for Elastic net, a hybrid of L1L1 and L2L2 regularization proposed in Zou et al, Regularization and variable selection via the elastic net. Mathematically, it is defined as a convex combination of the L1L1 and the L2L2 regularization terms:\n",
    "    - α(λ‖w‖1)+(1−α)(λ2‖w‖22),α∈[0,1],λ≥0\n",
    "    - α(λ‖w‖1)+(1−α)(λ2‖w‖22),α∈[0,1],λ≥0\n",
    "- By setting αα properly, elastic net contains both L1L1 and L2L2 regularization as special cases. For example, if a linear regression model is trained with the elastic net parameter αα set to 11, it is equivalent to a Lasso model. On the other hand, if αα is set to 00, the trained model reduces to a ridge regression model. We implement Pipelines API for both linear regression and logistic regression with elastic net regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.683314913574\n",
      "0.666287575147\n",
      "0.621706854603\n",
      "0.612726524589\n",
      "0.60603479868\n",
      "0.603175068757\n",
      "0.596962153484\n",
      "0.594074303198\n",
      "0.590608924334\n",
      "0.589472457649\n",
      "0.588218777573\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_48ed8801c442a3d26e2e"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LogisticRegressionTrainingSummary provides a summary for a LogisticRegressionModel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Multinomial logistic regression'></a>\n",
    "- 2.1.2 Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 X 4 CSRMatrix\n",
      "(0,3) 0.3176\n",
      "(1,2) -0.7804\n",
      "(1,3) -0.377\n",
      "Intercept: [0.0516523165983,-0.123912249909,0.0722599333102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext \\\n",
    "    .read \\\n",
    "    .format(\"libsvm\") \\\n",
    "    .load(\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multinomial logistic regression\n",
    "- Multiclass classification is supported via multinomial logistic (softmax) regression. In multinomial logistic regression, the algorithm produces K sets of coefficients, or a matrix of dimension K×J where K is the number of outcome classes and J is the number of features. If the algorithm is fit with an intercept term then a length K vector of intercepts is available.\n",
    "\n",
    "- Multinomial coefficients are available as coefficientMatrix and intercepts are available as interceptVector.\n",
    "- coefficients and intercept methods on a logistic regression model trained with multinomial family are not supported. Use coefficientMatrix and interceptVector instead.\n",
    "- The conditional probabilities of the outcome classes k∈1,2,…,Kk∈1,2,…,K are modeled using the softmax function.\n",
    "\n",
    "    - P(Y=k|X,βk,β0k)=eβk⋅X+β0k∑K−1k′=0eβk′⋅X+β0k′\n",
    "    - P(Y=k|X,βk,β0k)=eβk⋅X+β0k∑k′=0K−1eβk′⋅X+β0k′\n",
    "\n",
    "- We minimize the weighted negative log-likelihood, using a multinomial response model, with elastic-net penalty to control for overfitting.\n",
    "\n",
    "     -  minβ,β0−[∑i=1Lwi⋅logP(Y=yi|xi)]+λ[12(1−α)||β||22+α||β||1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Decision tree classifier'></a>\n",
    "- **2.2 Decision tree classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[95,96,97,12...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4eb184f930841a71fb90) of depth 2 with 5 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n",
    "\n",
    "- The spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test error is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Random forest classifier'></a>\n",
    "- ** 2.3 Random forest classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[122,123,124...|\n",
      "|           0.0|  0.0|(692,[123,124,125...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           0.0|  0.0|(692,[126,127,128...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_47caa25b78b06d7f6ecc) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forests are ensembles of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Gradient-boosted tree classifier'></a>\n",
    "- ** 2.4 Gradient-boosted tree classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[121,122,123...|\n",
      "|       1.0|         1.0|(692,[122,123,148...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.030303\n",
      "GBTClassificationModel (uid=GBTClassifier_4fe7a651de3bd359cfdc) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Linear Support Vector Machine'></a>\n",
    "- ** 2.5  Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000517063031747,-0.000117228865497,-8.88275483692e-05,8.52236071019e-05,0.0,0.0,-1.34363612633e-05,0.000372956980134,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000888894955263,0.000298640597618,0.000379337881619,-0.000176232889825,0.0,1.50284892697e-06,1.80560411449e-06,1.80287632604e-06,-3.38437135065e-06,-4.04158018481e-06,2.0965017727e-06,8.53611164299e-05,0.000220641774296,0.000216775999406,-0.000547240139656,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000921415502407,0.000313510668869,0.000248198431841,0.0,-4.14773819764e-05,-3.68321503845e-05,0.0,-3.96523661846e-06,-5.1569169805e-05,-6.62469728708e-05,-2.18214865042e-05,1.16344296907e-05,-1.1535211417e-06,3.81389604889e-05,1.58237116343e-06,-4.78401343234e-05,-9.38649322411e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000431748978271,0.000170554928674,0.0,-2.79782041361e-05,-5.88745220385e-05,-4.18587945298e-05,-3.74069296488e-05,-3.97879393049e-05,-5.54588189501e-05,-4.50501559842e-05,-3.21400249475e-06,-1.65618688083e-06,-4.41606398762e-06,-7.99861833153e-06,-4.72996211254e-05,-2.51659562591e-05,-3.64078092792e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000247190981306,0.0,-3.27063743138e-05,-5.57034078757e-05,-5.23368921257e-05,-7.82960448237e-05,-7.60385448388e-05,-8.37105130135e-05,-1.86695587538e-05,0.0,1.20453094862e-05,-2.3374084977e-05,-1.07886416889e-05,-5.57311944316e-05,-7.95297903359e-05,-1.45291967755e-05,8.73794834813e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0012589360773,-0.000181622863021,-0.000106507116646,-6.04035552771e-05,-4.85639297392e-05,-8.97389595465e-05,-8.78131677062e-05,-5.68487774674e-05,-3.78092673428e-05,1.38348970366e-05,7.58548512944e-05,5.50174118168e-05,-1.54307553982e-05,-1.83492870363e-05,-0.000103540082656,-0.000135278477214,-0.000112450076477,-2.93739160568e-05,-7.31121784734e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000285822861386,-0.000129981739714,-0.000147840802132,-8.20337460587e-05,-6.55668532001e-05,-5.63926603866e-05,-6.99557162733e-05,-4.66434815986e-05,-2.30265936988e-05,7.39883397917e-05,0.000148171761301,0.000109383174355,7.94042516701e-05,-6.74329480435e-07,-0.000126233027215,-0.000191103873554,-0.00018611622109,-0.000127767662547,-8.93530280652e-05,-1.23941723044e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000282953083135,-0.000139121896005,-0.000125931364646,-5.96474518793e-05,-5.36032815234e-05,-0.000105178806621,-0.00013856124131,-7.18103297413e-05,2.32490388651e-06,0.000156696426957,0.00023261206954,0.000172616382323,0.000138575309603,-1.39629902887e-05,-0.000157657739824,-0.00020728798812,-0.00019106441272,-0.000127448341614,-0.000127556116303,-5.18855915605e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000159081567023,-0.000121653123029,-5.62385107981e-05,-3.87798712638e-05,-7.55090050996e-05,-0.000107031400055,-0.000147204281381,-8.78142337451e-05,7.94165560942e-05,0.000232063549862,0.000275069823437,0.000254672223319,0.000181082166639,-1.30699166899e-05,-0.000184237422089,-0.000197754048245,-0.000177220740637,-0.000148798701472,-0.000118790214313,-9.75528388779e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000130274031136,-5.36830302355e-05,-1.76312000137e-05,-7.84661103461e-05,-0.000122100767283,-0.000172819685334,-0.000155923461289,-5.23957949291e-05,0.000168071934354,0.000289300867865,0.000362992149323,0.000295822351227,0.000217704669554,-6.40884808189e-05,-0.00019058225556,-0.000204251385646,-0.00017119949037,-0.000138534867983,-0.000130185929509,-0.000118877795128,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.02141111229e-05,-1.69450084317e-05,-7.18972282417e-05,-0.000145608280043,-0.000149354973406,-0.000194964193408,-0.000173837434173,-3.3438825792e-05,0.000286653832795,0.000298123215707,0.000377250607691,0.000321170282749,0.000257799511518,-0.000166273856567,-0.000180371058515,-0.000204193563442,-0.000179622372034,-0.000137264880836,-0.000134610144737,-0.000122642164692,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00152397525147,-5.47233086599e-05,-9.65684394936e-05,-0.000134247298535,-0.000147274677996,-0.000161627097882,-0.0001845825901,-0.000196996471351,0.000130852612943,0.000294317885711,0.000309777369283,0.000411283476931,0.00034113620757,0.000165299459244,-0.000210654108627,-0.000188392408154,-0.000197958641457,-0.000176213118722,-0.000127234362268,-0.000127081617192,-0.000148122210119,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00114068060054,-0.000132346742127,-0.000129046078543,-0.000141047485449,-0.00015194605434,-0.000211045393898,-0.00017911827582,-0.000189529482772,0.000217675715525,0.000302017916563,0.00040028632744,0.000403228067564,0.000411807738261,3.79174052529e-06,-0.000198862906602,-0.000195474431129,-0.000198573482187,-0.000133368922007,-0.000128301292929,-0.000118559163174,-0.000176559720376,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00109387695923,-0.000127854753052,-0.000134246997775,-0.000150520065248,-0.000193332878229,-0.000203851600866,-0.000174224706988,4.63598443911e-05,0.000206176230871,0.000286288289113,0.000407483098836,0.000372635778515,0.000350752019073,-0.000151648549436,-0.000170537519215,-0.000196389646544,-0.000199625862658,-0.000136123126643,-0.000121828553389,-0.000111667120816,-0.000137728388818,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000304438626012,-0.00012408366432,-0.000133531749272,-0.000157834426046,-0.000191684342434,-0.000187103227339,-0.000112839892315,0.000111365044531,0.000187072448927,0.00028654279529,0.00040032117545,0.000316963753631,0.000201589942787,-0.000131393928446,-0.000151810704824,-0.000182543184598,-0.000160253992857,-0.000132304047954,-0.000116691386913,-0.000105321549642,-0.000137090370424,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00040287410145,-0.000135639879509,-0.00013225887084,-0.000165235023898,-0.000201750742847,-0.000157245910639,2.57753650128e-06,0.000131246366342,0.000207074222919,0.000390810655443,0.000334870583299,0.000257904413672,2.6881819648e-05,-0.000151138358671,-0.000160542813933,-0.000172672874629,-0.000119389437681,-0.000105052450386,-0.00011109385509,-0.000134699142749,-0.00020735223736,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000503437423391,-0.000159612136884,-0.000127422212381,-0.000158282110488,-0.000213012206163,-0.00012933366375,1.68026731022e-05,0.000110209180827,0.000211607952727,0.000348734210508,0.000264872119444,0.000115160683503,-5.46827313969e-05,-0.000136320016309,-0.000143404058577,-0.000124869577382,-8.46287324798e-05,-9.58070841477e-05,-0.000107491666054,-0.000146180384592,-0.000375564462962,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000512434261188,-0.000203697340991,-0.000136269850983,-0.000133137681833,-0.000187155553782,-0.000118881731579,-1.87748175956e-05,5.7108412195e-05,0.000127281610561,0.000190214582149,0.000121773978959,-1.24611535743e-05,-7.55396181049e-05,-0.000102421745594,-4.44873554196e-05,-9.05856157796e-05,-6.83734719886e-05,-8.08440930426e-05,-0.000133168682996,-0.000203359163976,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000396651092847,-0.000137389836291,-3.79712214097e-05,-6.43176303557e-05,-0.000118577398823,-9.35952086311e-05,-5.08783715162e-05,-8.26936759509e-08,0.0,1.34345391311e-05,-1.96016902137e-06,-2.85270459905e-05,-7.41033269931e-05,-7.13213057008e-05,-4.97809611855e-05,-6.64150536138e-05,-6.96200551409e-05,-7.75289815833e-05,-0.000173936094992,-0.00125294792554,0.0,0.0,0.000206825212699,0.0,0.0,0.0,0.0,0.0,-0.000467024673836,-0.000103180363888,1.20044087858e-05,0.0,-2.51586393577e-05,-1.20952409108e-05,-5.19052816902e-06,-4.91679063956e-06,-8.48395853564e-06,-9.36275709707e-06,-2.09593357128e-05,-4.77900910439e-05,-7.92797600959e-05,-4.46268704178e-05,-4.18299242858e-05,-3.75479962859e-05,-4.52754480226e-05,-1.85535625615e-05,-0.000247630379621,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000348861804552,-5.68752365936e-06,7.38004027965e-05,4.3958606367e-05,7.14519824238e-05,6.18124834337e-06,0.0,-6.08555380835e-05,-4.85639083233e-05,-4.11792058893e-05,-4.35928362311e-05,-6.6087541615e-05,-5.44303225127e-05,-2.7782637881e-05,0.0,0.0,0.000287946139346,-0.00289555297779,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000123121148378,-1.95267479173e-05,-1.699950683e-05,5.48352941481e-05,1.52344163276e-05,-5.83656045253e-05,-0.000123781942165,-0.000117507049533,-6.19711523061e-05,-5.04200964581e-05,-0.000140552602236,-0.000141033094247,-0.000192723082389,-0.000480248996468]\n",
      "Intercept: 0.0129113052145\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linearsSVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. LinearSVC in Spark ML supports binary classification with linear SVM. Internally, it optimizes the Hinge Loss using OWLQN optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'One-vs-Rest classifier'></a>\n",
    "- **2.6 One-vs-Rest classifier (a.k.a. One-vs-All)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0909091\n",
      "GBTClassificationModel (uid=GBTClassifier_4fe7a651de3bd359cfdc) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# load data file.\n",
    "inputData = sqlContext.read.format(\"libsvm\") \\\n",
    "    .load(\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# generate the train/test split.\n",
    "(train, test) = inputData.randomSplit([0.8, 0.2])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The one-vs.-rest[2]:182, 338 (or one-vs.-all, OvA or OvR, one-against-all, OAA) strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a class label; discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample.[2]:182[note 1]\n",
    "\n",
    "- In pseudocode, the training algorithm for an OvA learner constructed from a binary classification learner L is as follows:\n",
    "\n",
    "- Inputs:\n",
    "    - L, a learner (training algorithm for binary classifiers)\n",
    "    - samples X\n",
    "    - labels y where yi ∈ {1, … K} is the label for the sample Xi\n",
    "- Output:\n",
    "    - a list of classifiers fk for k ∈ {1, …, K}\n",
    "- Procedure:\n",
    "    - For each k in {1, …, K}\n",
    "    - Construct a new label vector z where zi = 1 if yi = k and zi = 0 otherwise\n",
    "    - Apply L to X, z to obtain fk\n",
    "- Making decisions means applying all classifiers to an unseen sample x and predicting the label k for which the corresponding classifier reports the highest confidence score:\n",
    "    - https://wikimedia.org/api/rest_v1/media/math/render/svg/8c0fb3a3d9afaaa9baa4a0701f49ff52e3121037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Naive Bayes'></a>\n",
    "- ** 2.7 Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|label|            features|       rawPrediction|probability|prediction|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[-174115.98587057...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[98,99,100,1...|[-178402.52307196...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[100,101,102...|[-100905.88974016...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-244784.29791241...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-196900.88506109...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-238164.45338794...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-184206.87833381...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-214174.52863813...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-182844.62193963...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[-246557.10990301...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-208282.08496711...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-243457.69885665...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[-260933.50931276...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[-220274.72552901...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[-154830.07125175...|  [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-145978.24563975...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[100,101,102...|[-147916.32657832...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-139663.27471685...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-129013.44238751...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-81829.799906049...|  [0.0,1.0]|       1.0|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = sqlContext.read.format(\"libsvm\") \\\n",
    "    .load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regrestion\n",
    "<a id = 'Linear regression'></a>\n",
    " - ** 3.1 Linear regression**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.322925166774,-0.343854803456,1.91560170235,0.0528805868039,0.76596272046,0.0,-0.151053926692,-0.215879303609,0.220253691888]\n",
      "Intercept: 0.159893684424\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Generalized linear regression'></a>\n",
    "**3.1.1Generalized linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0105418280813,0.800325310056,-0.784516554142,2.36798871714,0.501000208986,1.12223511598,-0.292682439862,-0.498371743232,-0.603579718068,0.672555006719]\n",
      "Intercept: 0.145921761452\n",
      "Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.8312649247659919, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]\n",
      "T Values: [0.013259446542269243, 0.9942283563442594, -0.9836067393599172, 2.848657084633759, 0.6305509179635714, 1.382234441029355, -0.3695715687490668, -0.6250446546128238, -0.7271418403049983, 0.8654306337661122, 0.31453393176593286]\n",
      "P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]\n",
      "Dispersion: 105.609883568\n",
      "Null Deviance: 53229.3654339\n",
      "Residual Degree Of Freedom Null: 500\n",
      "Deviance: 51748.8429484\n",
      "Residual Degree Of Freedom: 490\n",
      "AIC: 3769.18958718\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-10.974359174246889|\n",
      "| 0.8872320138420559|\n",
      "| -4.596541837478908|\n",
      "|-20.411667435019638|\n",
      "|-10.270419345342642|\n",
      "|-6.0156058956799905|\n",
      "|-10.663939415849267|\n",
      "| 2.1153960525024713|\n",
      "| 3.9807132379137675|\n",
      "|-17.225218272069533|\n",
      "| -4.611647633532147|\n",
      "| 6.4176669407698546|\n",
      "| 11.407137945300537|\n",
      "| -20.70176540467664|\n",
      "| -2.683748540510967|\n",
      "|-16.755494794232536|\n",
      "|  8.154668342638725|\n",
      "|-1.4355057987358848|\n",
      "|-0.6435058688185704|\n",
      "|  -1.13802589316832|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Load training data\n",
    "dataset = sqlContext.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Decision tree regression'></a>\n",
    "**3.2 Decision tree regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[98,99,100,1...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.254\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_442faac1c27a664e208e) of depth 1 with 3 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ** 3.3 Random forest regression**\n",
    "    <a id = 'Random forest regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.1|  0.0|(692,[124,125,126...|\n",
      "|      0.15|  0.0|(692,[126,127,128...|\n",
      "|      0.05|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.111124\n",
      "RandomForestRegressionModel (uid=RandomForestRegressor_492ba9e43affd22e28a4) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ** 3.4 Gradient-boosted tree regression**\n",
    "     <a id = 'Gradient-boosted tree regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0\n",
      "GBTRegressionModel (uid=GBTRegressor_491bb9e74cea1982b50e) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ** 3.5 Survival regression**\n",
    "    <a id = 'Survival regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.496304411053,0.198452172529]\n",
      "Intercept: 2.63808989631\n",
      "Scale: 1.54723635336\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|label|censor|features      |prediction        |quantiles                              |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|1.218|1.0   |[1.56,-0.605] |5.718985621018949 |[1.1603229908059511,4.99546058340675]  |\n",
      "|2.949|0.0   |[0.346,2.158] |18.07678210850554 |[3.6675919944963185,15.789837303662035]|\n",
      "|3.627|0.0   |[1.38,0.231]  |7.381908879359964 |[1.4977129086101577,6.448002719505493] |\n",
      "|0.273|1.0   |[0.52,1.151]  |13.577717814884505|[2.7547784147915118,11.859962351993198]|\n",
      "|4.199|0.0   |[0.795,-0.226]|9.013087597344809 |[1.8286621877331872,7.872816406785483] |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "training = sqlContext.createDataFrame([\n",
    "    (1.218, 1.0, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, 0.0, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, 0.0, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, 1.0, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, 0.0, Vectors.dense(0.795, -0.226))], [\"label\", \"censor\", \"features\"])\n",
    "quantileProbabilities = [0.3, 0.6]\n",
    "aft = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "\n",
    "model = aft.fit(training)\n",
    "\n",
    "# Print the coefficients, intercept and scale parameter for AFT survival regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "print(\"Scale: \" + str(model.scale))\n",
    "model.transform(training).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ** 3.6 Isotonic regression**\n",
    "    <a id = 'Isotonic regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundaries in increasing order: [0.01,0.17,0.18,0.27,0.28,0.29,0.3,0.31,0.34,0.35,0.36,0.41,0.42,0.71,0.72,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,1.0]\n",
      "\n",
      "Predictions associated with the boundaries: [0.157152712941,0.157152712941,0.189138196,0.189138196,0.20040796,0.29576747,0.43396226,0.5081591025,0.5081591025,0.54156043,0.550484446667,0.550484446667,0.563929967,0.563929967,0.566037736667,0.566037736667,0.56603774,0.57929628,0.64762876,0.66241713,0.67210607,0.67210607,0.674655785,0.674655785,0.73890872,0.73992861,0.84242733,0.89673636,0.89673636,0.90719021,0.9272055075,0.9272055075]\n",
      "\n",
      "+----------+--------------+-------------------+\n",
      "|     label|      features|         prediction|\n",
      "+----------+--------------+-------------------+\n",
      "|0.24579296|(1,[0],[0.01])|0.15715271294117644|\n",
      "|0.28505864|(1,[0],[0.02])|0.15715271294117644|\n",
      "|0.31208567|(1,[0],[0.03])|0.15715271294117644|\n",
      "|0.35900051|(1,[0],[0.04])|0.15715271294117644|\n",
      "|0.35747068|(1,[0],[0.05])|0.15715271294117644|\n",
      "|0.16675166|(1,[0],[0.06])|0.15715271294117644|\n",
      "|0.17491076|(1,[0],[0.07])|0.15715271294117644|\n",
      "| 0.0418154|(1,[0],[0.08])|0.15715271294117644|\n",
      "|0.04793473|(1,[0],[0.09])|0.15715271294117644|\n",
      "|0.03926568| (1,[0],[0.1])|0.15715271294117644|\n",
      "|0.12952575|(1,[0],[0.11])|0.15715271294117644|\n",
      "|       0.0|(1,[0],[0.12])|0.15715271294117644|\n",
      "|0.01376849|(1,[0],[0.13])|0.15715271294117644|\n",
      "|0.13105558|(1,[0],[0.14])|0.15715271294117644|\n",
      "|0.08873024|(1,[0],[0.15])|0.15715271294117644|\n",
      "|0.12595614|(1,[0],[0.16])|0.15715271294117644|\n",
      "|0.15247323|(1,[0],[0.17])|0.15715271294117644|\n",
      "|0.25956145|(1,[0],[0.18])|        0.189138196|\n",
      "|0.20040796|(1,[0],[0.19])|        0.189138196|\n",
      "|0.19581846| (1,[0],[0.2])|        0.189138196|\n",
      "+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "# Loads data.\n",
    "dataset = sqlContext.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_isotonic_regression_libsvm_data.txt\")\n",
    "\n",
    "# Trains an isotonic regression model.\n",
    "model = IsotonicRegression().fit(dataset)\n",
    "print(\"Boundaries in increasing order: %s\\n\" % str(model.boundaries))\n",
    "print(\"Predictions associated with the boundaries: %s\\n\" % str(model.predictions))\n",
    "\n",
    "# Makes predictions.\n",
    "model.transform(dataset).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do classification and build regression model for csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "<a id = 'Binary Classification'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+\n",
      "|Age|Marital_Status|Gender|Weight_Category|Cholesterol|Stress_Management|Trait_Anxiety|2nd_Heart_Attack|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+\n",
      "| 60|             2|     0|              1|        150|                1|           50|             Yes|\n",
      "| 69|             2|     1|              1|        170|                0|           60|             Yes|\n",
      "| 52|             1|     0|              0|        174|                1|           35|              No|\n",
      "| 66|             2|     1|              1|        169|                0|           60|             Yes|\n",
      "| 70|             3|     0|              1|        237|                0|           65|             Yes|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "rawdata = sqlContext.read.load(\"data/heartattack_train.csv\", format=\"csv\", header=True)\n",
    "rawdata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+------------------------+\n",
      "|Age|Marital_Status|Gender|Weight_Category|Cholesterol|Stress_Management|Trait_Anxiety|2nd_Heart_Attack|indexed_2nd_Heart_Attack|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+------------------------+\n",
      "| 60|             2|     0|              1|        150|                1|           50|             Yes|                     1.0|\n",
      "| 69|             2|     1|              1|        170|                0|           60|             Yes|                     1.0|\n",
      "| 52|             1|     0|              0|        174|                1|           35|              No|                     0.0|\n",
      "| 66|             2|     1|              1|        169|                0|           60|             Yes|                     1.0|\n",
      "| 70|             3|     0|              1|        237|                0|           65|             Yes|                     1.0|\n",
      "| 52|             1|     0|              0|        174|                1|           35|              No|                     0.0|\n",
      "| 58|             2|     1|              0|        140|                0|           45|              No|                     0.0|\n",
      "| 59|             2|     1|              0|        143|                0|           45|             Yes|                     1.0|\n",
      "| 60|             2|     0|              0|        139|                0|           45|              No|                     0.0|\n",
      "| 51|             1|     1|              0|        174|                1|           40|              No|                     0.0|\n",
      "| 52|             1|     0|              0|        189|                1|           65|              No|                     0.0|\n",
      "| 70|             2|     1|              1|        147|                1|           50|             Yes|                     1.0|\n",
      "| 52|             2|     1|              2|        160|                0|           40|             Yes|                     1.0|\n",
      "| 74|             3|     1|              2|        178|                0|           75|             Yes|                     1.0|\n",
      "| 64|             2|     1|              2|        236|                1|           80|             Yes|                     1.0|\n",
      "| 69|             2|     0|              1|        146|                1|           50|             Yes|                     1.0|\n",
      "| 58|             2|     0|              0|        141|                0|           45|              No|                     0.0|\n",
      "| 68|             1|     0|              0|        172|                0|           60|              No|                     0.0|\n",
      "| 66|             1|     0|              0|        172|                0|           60|              No|                     0.0|\n",
      "| 63|             0|     1|              1|        138|                1|           50|              No|                     0.0|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+----------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# build indexer\n",
    "string_indexer = StringIndexer(inputCol='2nd_Heart_Attack', outputCol='indexed_2nd_Heart_Attack')\n",
    "\n",
    "# learn the model\n",
    "string_indexer_model = string_indexer.fit(rawdata)\n",
    "\n",
    "# transform the data\n",
    "rawdata = string_indexer_model.transform(rawdata)\n",
    "\n",
    "# resulting df\n",
    "rawdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop 2nd_Heart_Attack column\n",
    "drop_list = ['2nd_Heart_Attack']\n",
    "# Create new dataset named df\n",
    "df = rawdata.select([column for column in rawdata.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since 2nd_Heart_Attack is no numeral column, we creat index_2nd_Heart_Attack column change yes to 1, and no to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "|Age|Marital_Status|Gender|Weight_Category|Cholesterol|Stress_Management|Trait_Anxiety|indexed_2nd_Heart_Attack|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "| 60|             2|     0|              1|        150|                1|           50|                     1.0|\n",
      "| 69|             2|     1|              1|        170|                0|           60|                     1.0|\n",
      "| 52|             1|     0|              0|        174|                1|           35|                     0.0|\n",
      "| 66|             2|     1|              1|        169|                0|           60|                     1.0|\n",
      "| 70|             3|     0|              1|        237|                0|           65|                     1.0|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: string, Marital_Status: string, Gender: int, Weight_Category: string, Cholesterol: string, Stress_Management: string, Trait_Anxiety: string, indexed_2nd_Heart_Attack: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when   \n",
    "# change all null cell to 0\n",
    "df.withColumn('Age', when(rawdata.Age.isNotNull(), 1).otherwise(0))\n",
    "df.withColumn('Marital_Status', when(rawdata.Marital_Status.isNotNull(), 1).otherwise(0))\n",
    "df.withColumn('Gender', when(rawdata.Gender.isNotNull(), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "|Age|Marital_Status|Gender|Weight_Category|Cholesterol|Stress_Management|Trait_Anxiety|indexed_2nd_Heart_Attack|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "| 60|             2|     0|              1|        150|                1|           50|                     1.0|\n",
      "| 69|             2|     1|              1|        170|                0|           60|                     1.0|\n",
      "| 52|             1|     0|              0|        174|                1|           35|                     0.0|\n",
      "| 66|             2|     1|              1|        169|                0|           60|                     1.0|\n",
      "| 70|             3|     0|              1|        237|                0|           65|                     1.0|\n",
      "+---+--------------+------+---------------+-----------+-----------------+-------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: double, indexed_2nd_Heart_Attack: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Javascript\n",
    "display(df.select(\"Gender\", \"indexed_2nd_Heart_Attack\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Age', 'string')\n",
      "('Marital_Status', 'string')\n",
      "('Gender', 'string')\n",
      "('Weight_Category', 'string')\n",
      "('Cholesterol', 'string')\n",
      "('Stress_Management', 'string')\n",
      "('Trait_Anxiety', 'string')\n",
      "('indexed_2nd_Heart_Attack', 'double')\n"
     ]
    }
   ],
   "source": [
    "# Check the types\n",
    "for type in df.dtypes:\n",
    "    print type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The DataFrame is currently using strings, but we know all columns are numeric. Let's cast them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Marital_Status: double (nullable = true)\n",
      " |-- Gender: double (nullable = true)\n",
      " |-- Weight_Category: double (nullable = true)\n",
      " |-- Cholesterol: double (nullable = true)\n",
      " |-- Stress_Management: double (nullable = true)\n",
      " |-- Trait_Anxiety: double (nullable = true)\n",
      " |-- indexed_2nd_Heart_Attack: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col  # for indicating a column using a string in the line below\n",
    "df = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cast all columns to numeral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[60.0,2.0,0.0,1.0...|  1.0|\n",
      "|[69.0,2.0,1.0,1.0...|  1.0|\n",
      "|[52.0,1.0,0.0,0.0...|  0.0|\n",
      "|[66.0,2.0,1.0,1.0...|  1.0|\n",
      "|[70.0,3.0,0.0,1.0...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectors column 0 to column 7 as features, except last column ad lable\n",
    "from pyspark.ml.linalg import Vectors\n",
    "ad_df = df.rdd.map(lambda x: [Vectors.dense(x[0:7]), x[-1]]).toDF(['features', 'label'])\n",
    "ad_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vectorize column 0 to column 7as features and whether has and_Heart_Attack as label. use these features and label to do classification, and build the regression \n",
    "- When build the regression model, lable is y value, and select some other column as X variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LogisticRegression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = ad_df.randomSplit([0.7, 0.3], seed = 100)\n",
    "print trainingData.count()\n",
    "print testData.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502024291497976"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print lr.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Cross Validation for LogisticRegression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that cross-validation over a grid of parameters is expensive. E.g., in the example below, the parameter grid has 3 values for hashingTF.numFeatures and 2 values for lr.regParam, and CrossValidator uses 2 folds. This multiplies out to (3×2)×2=12 different models being trained. In realistic settings, it can be common to try many more parameters and use more folds (k=3k=3 and k=10k=10 are common). In other words, using CrossValidator can be very expensive. However, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=5 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441295546558704"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use BinaryClassificationEvaluator show the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  -0.194156014441\n",
      "Model coefficients [0.000401148326002,0.0,0.0,0.0118565832633,0.000114100948009,0.0,0.000235420351425]\n"
     ]
    }
   ],
   "source": [
    "print 'Model Intercept: ', cvModel.bestModel.intercept\n",
    "print 'Model coefficients',cvModel.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Feature Weight|\n",
      "+--------------------+\n",
      "|4.011483260021925E-4|\n",
      "|                 0.0|\n",
      "|                 0.0|\n",
      "|0.011856583263294649|\n",
      "|1.141009480086311...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on Spark 2.X weights are available as ceofficients\n",
    "weights = cvModel.bestModel.coefficients\n",
    "weights = map(lambda w: (float(w),), weights)  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "weightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **DecisionTreeClassification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "\n",
    "# Train model with Training Data\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  7\n",
      "depth =  3\n"
     ]
    }
   ],
   "source": [
    "print \"numNodes = \", dtModel.numNodes\n",
    "print \"depth = \", dtModel.depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7 nodes in tree, including leaf nodes.\n",
    "- depth: Get depth of tree. E.g.: Depth 0 means 1 leaf node. Depth 1 means 1 internal node and 2 leaf nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = dtModel.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-------------+-----------+----------+\n",
      "|      features|label|rawPrediction|probability|prediction|\n",
      "+--------------+-----+-------------+-----------+----------+\n",
      "|[50.0,1.0,1.0]|  0.0|   [19.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,0.0]|  0.0|   [19.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,1.0]|  0.0|   [19.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,1.0]|  0.0|   [19.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[52.0,1.0,1.0]|  0.0|   [19.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+--------------+-----+-------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815789473684212"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gini'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.getImpurity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The impurity density p, i.e. the impurity content in a unit of control volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cross Validation for DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1,2,6,10])\n",
    "             .addGrid(dt.maxBins, [20,40,80])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# Takes ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  9\n",
      "depth =  4\n"
     ]
    }
   ],
   "source": [
    "print \"numNodes = \", cvModel.bestModel.numNodes\n",
    "print \"depth = \", cvModel.bestModel.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-----------+----------+\n",
      "|            features|label|rawPrediction|probability|prediction|\n",
      "+--------------------+-----+-------------+-----------+----------+\n",
      "|[50.0,1.0,1.0,0.0...|  0.0|   [34.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,0.0,0.0...|  0.0|   [34.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,1.0,0.0...|  0.0|   [34.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[51.0,1.0,1.0,0.0...|  0.0|   [34.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[52.0,1.0,1.0,0.0...|  0.0|   [34.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+--------------------+-----+-------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8886639676113361"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765182186234818"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Cross Validation for Random Forest classificaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will try tuning the model with the ParamGridBuilder and the CrossValidator.\n",
    "\n",
    "- If you are unsure what params are available for tuning, you can use explainParams() to print a list of all params and their definitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[50.0,1.0,1.0,0.0...|  0.0|        [19.92,0.08]|[0.99600000000000...|       0.0|\n",
      "|[51.0,1.0,0.0,0.0...|  0.0|[19.8575,0.142500...|[0.99287500000000...|       0.0|\n",
      "|[51.0,1.0,1.0,0.0...|  0.0|        [19.92,0.08]|[0.99600000000000...|       0.0|\n",
      "|[51.0,1.0,1.0,0.0...|  0.0|        [19.92,0.08]|[0.99600000000000...|       0.0|\n",
      "|[52.0,1.0,1.0,0.0...|  0.0|        [19.92,0.08]|[0.99600000000000...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best model in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(ad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848739495798319"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalPredictions.createOrReplaceTempView(\"finalPredictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "<a id = 'Regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build regresion\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = lr.fit(ad_df)\n",
    "# evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split data into training and test datasets\n",
    "training, test = ad_df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##=====build cross valiation model======\n",
    "\n",
    "# estimator\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
    "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
    "    build()\n",
    "    \n",
    "# evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
    "\n",
    "# cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_test_cv = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6284529310846423"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on training data\n",
    "evaluator.evaluate(pred_training_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8009583558656643"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on test data\n",
    "evaluator.evaluate(pred_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8847381168828121"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.009, 0.1499, -0.03, 0.407, 0.0027, -0.0692, -0.0044])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best regParam: 0.0\n",
      "best ElasticNetParam:0.0\n"
     ]
    }
   ],
   "source": [
    "print('best regParam: ' + str(cv_model.bestModel._java_obj.getRegParam()) + \"\\n\" +\n",
    "     'best ElasticNetParam:' + str(cv_model.bestModel._java_obj.getElasticNetParam()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x119222190>,\n",
       "  <matplotlib.axis.XTick at 0x1191eab90>],\n",
       " <a list of 2 Text xticklabel objects>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1192b6190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "responses = df.groupBy('indexed_2nd_Heart_Attack').count().collect()\n",
    "categories = [i[0] for i in responses]\n",
    "counts = [i[1] for i in responses]\n",
    " \n",
    "ind = np.array(range(len(categories)))\n",
    "width = 0.35\n",
    "plt.bar(ind, counts, width=width, color='r')\n",
    " \n",
    "plt.ylabel('counts')\n",
    "plt.title('Response distribution')\n",
    "plt.xticks(ind + width/2., categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marital_Status', 4)\n",
      "('Gender', 2)\n",
      "('Weight_Category', 3)\n",
      "('Cholesterol', 55)\n",
      "('Stress_Management', 2)\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[1:-2]:\n",
    "    print(col, df.select(col).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|[49.0,1.0,1.0,0.0...|\n",
      "|       0.0|  0.0|[51.0,1.0,0.0,0.0...|\n",
      "|       0.0|  0.0|[52.0,1.0,0.0,0.0...|\n",
      "|       0.0|  0.0|[52.0,1.0,1.0,0.0...|\n",
      "|       1.0|  0.0|[52.0,1.0,1.0,1.0...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.257074\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_41c391d088236c432860) of depth 5 with 21 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(ad_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = ad_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GBTRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          prediction|label|            features|\n",
      "+--------------------+-----+--------------------+\n",
      "|-4.29326931031838...|  0.0|[42.0,1.0,0.0,0.0...|\n",
      "|0.003492694903010...|  0.0|[49.0,1.0,0.0,0.0...|\n",
      "|-4.29326931031838...|  0.0|[50.0,1.0,1.0,0.0...|\n",
      "|-4.29326931031838...|  0.0|[51.0,1.0,1.0,0.0...|\n",
      "|-4.29326931031838...|  0.0|[52.0,1.0,0.0,0.0...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.268009\n",
      "GBTRegressionModel (uid=GBTRegressor_4e24acf88255e4bd7fe2) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **DecisionTreeRegression using rawdata set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 101 training examples and 37 test examples.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset randomly into 70% for training and 30% for testing.\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "print \"We have %d training examples and %d test examples.\" % (train.count(), test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "featuresCols = df.columns\n",
    "featuresCols.remove('indexed_2nd_Heart_Attack')\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all feature columns into a single feature vector in a new column \"rawFeatures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "# Takes the \"features\" column and learns to predict \"cnt\"\n",
    "gbt = GBTRegressor(labelCol=\"indexed_2nd_Heart_Attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n",
    "# In this example notebook, we keep these values small.  In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and more trees in the ensemble (>100).\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "  .addGrid(gbt.maxIter, [10, 100])\\\n",
    "  .build()\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we can tie our feature processing and model training stages together into a single Pipeline.\n",
    "- Train the Pipeline!\n",
    "- Now that we have set up our workflow, we can train the Pipeline in a single call. Calling fit() will run feature processing, model tuning, and training in a single call. We get back a fitted Pipeline with the best model found.\n",
    "   - Note: This next cell can take up to 10 minutes. This is because it is training a lot of trees:\n",
    "    - For each random sample of data in Cross Validation,\n",
    "    - For each setting of the hyperparameters,\n",
    "    - CrossValidator is training a separate GBT ensemble which contains many Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[indexed_2nd_Heart_Attack: double, prediction: double, Age: double, Marital_Status: double, Gender: double, Weight_Category: double, Cholesterol: double, Stress_Management: double, Trait_Anxiety: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.select(\"indexed_2nd_Heart_Attack\", \"prediction\", *featuresCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on our test set: 0.210957\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print \"RMSE on our test set: %g\" % rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving  model\n",
    "- This section describes how to take this notebook and improve the results even more. Try copying this notebook into your Databricks account and extending it, and see how much you can improve the predictions.\n",
    "- There are several ways we could further improve our model:\n",
    "- Expert knowledge: We may not be experts on bike sharing programs, but we know a few things we can use:\n",
    "- The count of rentals cannot be negative. GBTRegressor does not know that, but we could threshold the predictions to be >= 0 post-hoc.\n",
    "- The count of rentals is the sum of registered and casual rentals. These two counts may have different behavior. (Frequent cyclists and casual cyclists probably rent bikes for different reasons.) The best models for this dataset take this into account. Try training one GBT model for registered and one for casual, and then add their predictions together to get the full prediction.\n",
    "- Better tuning: To make this notebook run quickly, we only tried a few hyperparameter settings. To get the most out of our data, we should test more settings. Start by increasing the number of trees in our GBT model by setting maxIter=200; it will take longer to train but can be more accurate.\n",
    "- Feature engineering: We used the basic set of features given to us, but we could potentially improve them. For example, we may guess that weather is more or less important depending on whether or not it is a workday vs. weekend. To take advantage of that, we could build a few feature by combining those two base features. MLlib provides a suite of feature transformers; find out more in the ML guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    " - https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "- https://spark.apache.org/docs/latest/ml-classification-regression.html#regression\n",
    "    \n",
    "- https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "    \n",
    "- http://sean.lane.sh/blog/2016/PySpark_and_LDA\n",
    "    \n",
    "- https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231782/4413065072037724/latest.html\n",
    "    \n",
    "- https://stackoverflow.com/questions/29600673/how-to-delete-columns-in-pyspark-dataframe\n",
    "    \n",
    "- https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "    \n",
    "- https://weiminwang.blog/2016/06/09/pyspark-tutorial-building-a-random-forest-binary-classifier-on-unbalanced-dataset/\n",
    "\n",
    "- https://docs.databricks.com/spark/latest/mllib/decision-trees.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
